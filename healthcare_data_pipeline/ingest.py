#!/usr/bin/env python3
"""
FHIR Data Ingestion Script

Loads Synthea-generated FHIR bundles into MongoDB collections with support
for all major FHIR resource types. Implements comprehensive validation,
bulk operations, and automatic index creation.

This script processes FHIR R4 bundles generated by Synthea and ingests
them into MongoDB with proper error handling and progress reporting.
"""

import json
import logging
import sys
import time
from datetime import UTC, datetime
from pathlib import Path
from typing import Any

from pymongo import MongoClient, UpdateOne
from pymongo.errors import BulkWriteError, ConnectionFailure, ServerSelectionTimeoutError

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


# ANSI color codes for formatted output
class Colors:
    """ANSI color codes for terminal output."""

    OKGREEN = "\033[92m"
    WARNING = "\033[93m"
    FAIL = "\033[91m"
    ENDC = "\033[0m"
    BOLD = "\033[1m"
    OKBLUE = "\033[94m"


def print_success(msg: str) -> None:
    """Print success message with green color."""
    print(f"{Colors.OKGREEN}[OK]{Colors.ENDC} {msg}")


def print_error(msg: str) -> None:
    """Print error message with red color."""
    print(f"{Colors.FAIL}[ERROR]{Colors.ENDC} {msg}")


def print_warning(msg: str) -> None:
    """Print warning message with yellow color."""
    print(f"{Colors.WARNING}[WARN]{Colors.ENDC} {msg}")


def print_info(msg: str) -> None:
    """Print info message with blue color."""
    print(f"{Colors.OKBLUE}[INFO]{Colors.ENDC} {msg}")


def print_header(msg: str) -> None:
    """Print formatted header."""
    print(f"\n{Colors.BOLD}{'=' * 80}{Colors.ENDC}")
    print(f"{Colors.BOLD}{msg.center(80)}{Colors.ENDC}")
    print(f"{Colors.BOLD}{'=' * 80}{Colors.ENDC}\n")


def wait_for_mongodb(client: MongoClient, max_retries: int = 30, retry_delay: int = 2) -> bool:
    """
    Wait for MongoDB to be ready and responding to commands.

    Args:
        client: MongoDB client instance
        max_retries: Maximum number of retry attempts
        retry_delay: Delay in seconds between retries

    Returns:
        True if MongoDB is ready, False otherwise
    """
    print_info("Waiting for MongoDB to be ready...")

    for attempt in range(max_retries):
        try:
            client.admin.command("ping")
            print_success("MongoDB is ready")
            return True
        except (ConnectionFailure, ServerSelectionTimeoutError):
            if attempt < max_retries - 1:
                print_info(f"Attempt {attempt + 1}/{max_retries} - retrying in {retry_delay}s...")
                time.sleep(retry_delay)
            else:
                print_error("MongoDB connection timeout - max retries exceeded")
                return False
    return False


def get_mongodb_client(
    host: str = "localhost",
    port: int = 27017,
    user: str = "admin",
    password: str = "mongopass123",
    db_name: str = "text_to_mongo_db",
    timeout_ms: int = 30000,
) -> MongoClient:
    """
    Create and return MongoDB client with proper error handling.

    Args:
        host: MongoDB host
        port: MongoDB port
        user: MongoDB username
        password: MongoDB password
        db_name: Database name
        timeout_ms: Connection timeout in milliseconds

    Returns:
        Configured MongoClient instance

    Raises:
        ConnectionFailure: If unable to connect to MongoDB
    """
    connection_string = f"mongodb://{user}:{password}@{host}:{port}/{db_name}?authSource=admin"
    print_info(f"Connecting to MongoDB at {host}:{port}/{db_name}")

    client = MongoClient(
        connection_string, serverSelectionTimeoutMS=timeout_ms, connectTimeoutMS=timeout_ms
    )

    return client


def find_fhir_files(data_path: str) -> list[Path]:
    """
    Search for FHIR JSON files in common Synthea output locations.

    Args:
        data_path: Base path to search for FHIR files

    Returns:
        List of Path objects pointing to FHIR JSON files
    """
    base_path = Path(data_path)

    # Check common Synthea output locations
    search_paths = [
        base_path / "fhir",
        base_path / "fhir_r4",
        base_path / "output" / "fhir",
        base_path / "output" / "fhir_r4",
        base_path,
    ]

    fhir_files = []
    for search_path in search_paths:
        if search_path.exists():
            files = list(search_path.glob("*.json"))
            if files:
                print_info(f"Found {len(files)} FHIR files in {search_path}")
                fhir_files.extend(files)

    return fhir_files


def extract_resources_from_bundle(bundle: dict[str, Any]) -> list[dict[str, Any]]:
    """
    Extract all resources from a FHIR bundle.

    Args:
        bundle: FHIR Bundle resource as dictionary

    Returns:
        List of extracted resources with resourceType field
    """
    resources = []

    if bundle.get("resourceType") == "Bundle":
        entries = bundle.get("entry", [])
        for entry in entries:
            resource = entry.get("resource")
            if resource and resource.get("resourceType"):
                resources.append(resource)
    elif bundle.get("resourceType"):
        # Single resource file
        resources.append(bundle)

    return resources


def sanitize_resource(resource: dict[str, Any]) -> dict[str, Any]:
    """
    Sanitize FHIR resource for MongoDB insertion.

    Preserves FHIR IDs and adds metadata for tracking.

    Args:
        resource: FHIR resource dictionary

    Returns:
        Sanitized resource dictionary ready for MongoDB
    """
    resource_copy = dict(resource)

    # Store original FHIR ID
    if "id" in resource_copy:
        resource_copy["fhir_id"] = resource_copy["id"]

    # Add ingestion timestamp
    if "ingestion_metadata" not in resource_copy:
        resource_copy["ingestion_metadata"] = {
            "ingested_at": datetime.now(UTC).isoformat(),
            "ingestion_version": "1.0",
        }

    return resource_copy


def create_indexes(db) -> None:
    """
    Create indexes on MongoDB collections for optimized queries.

    Indexes are created for common query patterns in healthcare data
    analysis including lookups by resource ID, subject reference, and codes.

    Args:
        db: MongoDB database instance
    """
    print_info("Creating indexes for optimal query performance...")

    index_definitions = {
        "patients": [
            ("id", {}),
            ("identifier.value", {}),
        ],
        "encounters": [
            ("subject.reference", {}),
            ("period.start", {}),
            ("type.coding.code", {}),
        ],
        "conditions": [
            ("subject.reference", {}),
            ("code.coding.code", {}),
            ("code.coding.display", {}),
            ("clinicalStatus.coding.code", {}),
        ],
        "observations": [
            ("subject.reference", {}),
            ("code.coding.code", {}),
            ("effectiveDateTime", {}),
            ("value.Quantity.value", {}),
        ],
        "medicationrequests": [
            ("subject.reference", {}),
            ("medicationCodeableConcept.coding.display", {}),
            ("status", {}),
            ("authoredOn", {}),
        ],
        "allergyintolerances": [
            ("patient.reference", {}),
            ("code.coding.code", {}),
        ],
        "procedures": [
            ("subject.reference", {}),
            ("code.coding.code", {}),
            ("performedDateTime", {}),
        ],
        "immunizations": [
            ("patient.reference", {}),
            ("vaccineCode.coding.code", {}),
            ("occurrenceDateTime", {}),
        ],
        "careplans": [
            ("subject.reference", {}),
            ("status", {}),
        ],
        "diagnosticreports": [
            ("subject.reference", {}),
            ("code.coding.code", {}),
            ("effectiveDateTime", {}),
        ],
        "drugs": [
            ("ingredient_rxcui", {"unique": True}),
            ("primary_drug_name", {}),
            ("therapeutic_class_l2", {}),
            ("drug_class_l3", {}),
        ],
    }

    indexes_created = 0
    for collection_name, indexes in index_definitions.items():
        if collection_name in db.list_collection_names():
            collection = db[collection_name]
            for index_field, index_options in indexes:
                try:
                    collection.create_index(index_field, **index_options)
                    indexes_created += 1
                    logger.debug(f"Created index on {collection_name}.{index_field}")
                except Exception as e:
                    if "already exists" not in str(e).lower():
                        logger.warning(
                            f"Could not create index {index_field} on {collection_name}: {e}"
                        )

    print_success(f"Created {indexes_created} indexes")


def ingest_fhir_data(
    data_path: str,
    db_name: str = "text_to_mongo_db",
    host: str = "localhost",
    port: int = 27017,
    user: str = "admin",
    password: str = "mongopass123",
) -> dict[str, int]:
    """
    Main ingestion function that orchestrates the entire FHIR data loading process.

    Args:
        data_path: Path to directory containing FHIR JSON files
        db_name: MongoDB database name
        host: MongoDB host
        port: MongoDB port
        user: MongoDB username
        password: MongoDB password

    Returns:
        Dictionary with ingestion statistics
    """
    print_header("FHIR DATA INGESTION PIPELINE")

    # Connect to MongoDB
    client = get_mongodb_client(host, port, user, password, db_name)

    if not wait_for_mongodb(client):
        print_error("Cannot proceed without MongoDB connection")
        sys.exit(1)

    db = client[db_name]

    # Find FHIR files
    fhir_files = find_fhir_files(data_path)

    if not fhir_files:
        print_error(f"No FHIR files found in {data_path}")
        print_info("Expected directories: fhir/, fhir_r4/, output/fhir/, output/fhir_r4/")
        client.close()
        sys.exit(1)

    print_success(f"Found {len(fhir_files)} FHIR bundle files")

    # Resource type to collection name mapping
    resource_collections = {
        "Patient": "patients",
        "Encounter": "encounters",
        "Condition": "conditions",
        "Observation": "observations",
        "MedicationRequest": "medicationrequests",
        "AllergyIntolerance": "allergyintolerances",
        "Procedure": "procedures",
        "Immunization": "immunizations",
        "CarePlan": "careplans",
        "DiagnosticReport": "diagnosticreports",
        "Claim": "claims",
        "ExplanationOfBenefit": "explanationofbenefits",
    }

    # Initialize statistics
    stats = dict.fromkeys(resource_collections.values(), 0)
    stats["total_bundles"] = 0
    stats["total_resources"] = 0
    stats["errors"] = 0

    # Process each FHIR file
    print_info(f"Processing {len(fhir_files)} FHIR bundle files...")
    print()

    for idx, fhir_file in enumerate(fhir_files, 1):
        try:
            print_info(f"[{idx}/{len(fhir_files)}] Processing: {fhir_file.name}")

            # Read and parse FHIR bundle
            with open(fhir_file, encoding="utf-8") as f:
                bundle = json.load(f)

            resources = extract_resources_from_bundle(bundle)
            stats["total_bundles"] += 1

            if not resources:
                print_warning(f"No resources found in {fhir_file.name}")
                continue

            # Group resources by type
            resources_by_type: dict[str, list[dict[str, Any]]] = {}
            for resource in resources:
                resource_type = resource.get("resourceType")
                if resource_type in resource_collections:
                    if resource_type not in resources_by_type:
                        resources_by_type[resource_type] = []
                    resources_by_type[resource_type].append(sanitize_resource(resource))
                    stats["total_resources"] += 1

            # Bulk insert by resource type
            for resource_type, resources_list in resources_by_type.items():
                collection_name = resource_collections[resource_type]
                collection = db[collection_name]

                try:
                    # Create upsert operations based on FHIR ID
                    operations = [
                        UpdateOne(
                            {"fhir_id": resource.get("fhir_id")}, {"$set": resource}, upsert=True
                        )
                        for resource in resources_list
                    ]

                    if operations:
                        result = collection.bulk_write(operations, ordered=False)
                        inserted = result.upserted_count + result.modified_count
                        stats[collection_name] += inserted
                        logger.debug(
                            f"{resource_type}: {result.upserted_count} inserted, "
                            f"{result.modified_count} modified"
                        )

                except BulkWriteError as bwe:
                    print_warning(
                        f"Partial bulk write for {resource_type}: "
                        f"{bwe.details.get('nInserted', 0)} inserted"
                    )
                    stats[collection_name] += bwe.details.get("nInserted", 0)
                    stats["errors"] += 1

        except json.JSONDecodeError as e:
            print_error(f"Invalid JSON in {fhir_file.name}: {e}")
            stats["errors"] += 1
        except Exception as e:
            print_error(f"Error processing {fhir_file.name}: {e}")
            logger.exception(f"Detailed error for {fhir_file.name}")
            stats["errors"] += 1

    print()

    # Create indexes for optimized queries
    create_indexes(db)

    # Print summary statistics
    print_header("INGESTION COMPLETE")

    print_success(f"Processed {stats['total_bundles']} bundles")
    print_success(f"Ingested {stats['total_resources']} total resources")

    if stats["errors"] > 0:
        print_warning(f"Encountered {stats['errors']} errors during processing")

    print_info("\nResources by collection:")
    for collection_name in sorted(resource_collections.values()):
        count = stats[collection_name]
        if count > 0:
            print(f"  {Colors.OKGREEN}â—{Colors.ENDC} {collection_name}: {count}")

    print(f"\n{Colors.OKGREEN}Database: {db_name}{Colors.ENDC}")
    print(f"{Colors.OKGREEN}Host: {host}:{port}{Colors.ENDC}")
    print(f"{Colors.OKGREEN}Ready for queries!{Colors.ENDC}\n")

    # Close connection
    client.close()

    return stats


def ingest_drug_data(
    db_name: str = "text_to_mongo_db",
    host: str = "localhost",
    port: int = 27017,
    user: str = "admin",
    password: str = "mongopass123",
    request_delay: float = 0.3,
) -> dict[str, int]:
    """Ingest RxNav drug data with ATC classifications into MongoDB.

    Extracts drug ingredient and ATC classification data from the RxNav API,
    validates using Pydantic models, and ingests into the MongoDB 'drugs' collection.
    Uses upsert operations to ensure idempotency - safe to re-run without duplicates.

    Args:
        db_name: MongoDB database name
        host: MongoDB host
        port: MongoDB port
        user: MongoDB username
        password: MongoDB password
        request_delay: Seconds to wait between RxNav API requests

    Returns:
        Dictionary with ingestion statistics (drugs_ingested, errors)
    """
    from healthcare_data_pipeline.models import RxNavDrugModel
    from healthcare_data_pipeline.rxnav_extract import extract_drug_data

    print_header("DRUG DATA INGESTION PIPELINE (RXNAV)")

    # Connect to MongoDB
    client = get_mongodb_client(host, port, user, password, db_name)

    if not wait_for_mongodb(client):
        print_error("Cannot proceed without MongoDB connection")
        return {"drugs_ingested": 0, "errors": 1}

    db = client[db_name]
    collection = db["drugs"]

    # Initialize statistics
    stats = {"drugs_ingested": 0, "errors": 0, "validation_errors": 0}

    try:
        print_info("Extracting drug data from RxNav API...")
        print_warning("This may take several minutes on first run\n")

        # Extract drug data from RxNav API
        drug_data = extract_drug_data(request_delay=request_delay)

        if not drug_data:
            print_warning("No drug data retrieved from RxNav API")
            client.close()
            return stats

        print_success(f"Retrieved {len(drug_data)} drug records from RxNav")
        print_info("Validating and ingesting drug data...\n")

        # Validate and prepare records for insertion
        valid_records = []
        for idx, record in enumerate(drug_data, 1):
            try:
                # Validate using Pydantic model
                drug_model = RxNavDrugModel(**record)
                drug_model.add_ingestion_metadata()

                # Convert model to dictionary for MongoDB
                valid_records.append(drug_model.model_dump())

                if idx % 500 == 0:
                    print_info(f"Validated {idx}/{len(drug_data)} records")

            except Exception as e:
                logger.debug(f"Validation error for record {idx}: {e}")
                stats["validation_errors"] += 1
                stats["errors"] += 1

        print_success(f"Validated {len(valid_records)} records")

        if not valid_records:
            print_warning("No valid records after validation")
            client.close()
            return stats

        # Bulk upsert records to MongoDB
        print_info(f"Upserting {len(valid_records)} records to 'drugs' collection...")

        try:
            # Create upsert operations based on RXCUI (unique identifier)
            operations = [
                UpdateOne(
                    {"ingredient_rxcui": record.get("ingredient_rxcui")},
                    {"$set": record},
                    upsert=True,
                )
                for record in valid_records
            ]

            if operations:
                result = collection.bulk_write(operations, ordered=False)
                inserted = result.upserted_count + result.modified_count
                stats["drugs_ingested"] = inserted

                logger.debug(
                    f"Drugs: {result.upserted_count} upserted, {result.modified_count} modified"
                )

        except BulkWriteError as bwe:
            print_warning(
                f"Partial bulk write for drugs: {bwe.details.get('nInserted', 0)} inserted"
            )
            stats["drugs_ingested"] += bwe.details.get("nInserted", 0)
            stats["errors"] += 1

        print()

        # Print summary statistics
        print_header("DRUG INGESTION COMPLETE")

        print_success(f"Ingested {stats['drugs_ingested']} drug records")

        if stats["validation_errors"] > 0:
            print_warning(f"Encountered {stats['validation_errors']} validation errors")

        if stats["errors"] > 0:
            print_warning(f"Total errors: {stats['errors']}")
        else:
            print_success("No errors encountered")

        print(f"\n{Colors.OKGREEN}Collection: drugs{Colors.ENDC}")
        print(f"{Colors.OKGREEN}Database: {db_name}{Colors.ENDC}\n")

        return stats

    except Exception as e:
        print_error(f"Drug ingestion failed: {e}")
        logger.exception("Detailed error for drug ingestion")
        stats["errors"] += 1
        return stats

    finally:
        # Close connection
        client.close()


def main() -> None:
    """Main entry point for the ingestion script."""
    import argparse

    parser = argparse.ArgumentParser(
        description="Ingest Synthea FHIR data into MongoDB",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python ingest.py ./synthea_output/output/fhir
  python ingest.py /data/fhir --host mongodb.example.com
  python ingest.py ./data --db-name my_healthcare_db
        """,
    )

    parser.add_argument("data_path", help="Path to directory containing FHIR JSON files")
    parser.add_argument(
        "--db-name",
        default="text_to_mongo_db",
        help="MongoDB database name (default: text_to_mongo_db)",
    )
    parser.add_argument("--host", default="localhost", help="MongoDB host (default: localhost)")
    parser.add_argument("--port", type=int, default=27017, help="MongoDB port (default: 27017)")
    parser.add_argument("--user", default="admin", help="MongoDB username (default: admin)")
    parser.add_argument(
        "--password", default="mongopass123", help="MongoDB password (default: mongopass123)"
    )

    args = parser.parse_args()

    try:
        ingest_fhir_data(
            data_path=args.data_path,
            db_name=args.db_name,
            host=args.host,
            port=args.port,
            user=args.user,
            password=args.password,
        )
    except KeyboardInterrupt:
        print(f"\n{Colors.WARNING}Ingestion cancelled by user{Colors.ENDC}")
        sys.exit(130)
    except Exception as e:
        print_error(f"Fatal error: {e}")
        logger.exception("Detailed error information")
        sys.exit(1)


if __name__ == "__main__":
    main()
